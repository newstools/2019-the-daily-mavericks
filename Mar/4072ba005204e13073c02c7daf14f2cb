Photo by Markus Spiske on Unsplash Automated journalism is already here, and here to stay. Chinese state news agency Xinhua premiered its first robotic news anchor in November 2018 and will be releasing an updated female AI in a month’s time. In the West, Associated Press and other news agencies already use algorithms to publish business insights and short articles on the outcomes of sports matches. Multiple news sites use automated software to translate news in real time. Because of this onslaught of news automation, some might think that the robot revolution has already started. But in reality, machines have been able to talk and write like humans for years. Natural Language Processing is a field of machine learning that focuses on how computers interact with natural languages, in particular, the understanding and generation of language. So when OpenAI, an American-based NGO focused on developing artificial intelligence, released a statement in February 2019 claiming to have made a Natural Language Processing model too dangerous to release to the public, machine learning researchers were naturally confused. In a blog post about their new model, named GPT-2, OpenAI gave the following reason for not releasing their code: “Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a much smaller version of GPT-2 along with sampling code.” It appears OpenAI is worried that its programme, which can write coherent tales about English-speaking unicorns in just 10 attempts, will be used to spread disinformation campaigns and create fake news if released to the public. In many ways, the prospect of machines that can create natural-sounding “news” articles and publish them as “real” blogs or social media pages is worrisome. According to Doctor Geoff Nitschke, a senior lecturer at the University of Cape Town (UCT) and biological AI researcher, the fear of computers creating fake news is “legitimate”. “More computer processing power and larger data-sets mean that Natural Language Processing programmes can be quickly trained to effectively generate realistic-sounding text and speech,” Dr Nitschke told Daily Maverick. But how are machines actually trained? To start off with, you have to build a computer programme. Initially, programmes are just lines of code that don’t do much. In order to teach a machine, or create artificial intelligence, you have to feed it information. Natural Language Processing programmes require huge amounts of text — think terabytes of data — in order for a machine to understand how to write like a human. During the machine training process, a programme will be fed an input, to which it will produce an automated output. Developers train the programme by positively rewarding good or logical outputs, and negatively rewarding bad or nonsensical outputs. The more a programme is trained, the more likely it is to produce good outputs. For example, a model might be given input about weather. If it produces an output stating that “it started to rain inside the house”, it will be flagged as incorrect. The machine then starts to learn that it cannot talk about rain indoors. So if the machine learns what is right and what is wrong, how could it spread fake news? Take OpenAI’s GPT-2 programme as an example — the programme does not fact-check the information it produces as an output, it simply predicts the next word in a sentence and moves on. Generating a sentence is not actually that difficult, says Nitschke. Rather, generating a coherent programme that engages with the content it produces is the ultimate goal of Natural Language Processing, and from the information provided by OpenAI their GPT-2 programme has not achieved this goal. Julie Posetti, co-editor of Journalism, F*ke News and Disinformation, and Senior Research Fellow at the Reuters Institute for the Study of Journalism at the University of Oxford, told Daily Maverick that “AI technology, misused, can certainly make it easier to create content that is deliberately false and virtually indistinguishable from authentic content”.  1 In particular, machines given training data from news sources will inevitably mimic those news sources. High-quality fake news, in combination with automated Twitter accounts that make content appear popular, encourage real social media users to spread fictitious information. And in some instances, such as the automated chat bot named Tay that Microsoft released in 2016, real social media users encourage AI to spread fictitious information. In response to a tweet questioning comedian Ricky Gervais’s religious beliefs, Tay responded with “ricky gervais learned totalitarianism from adolf hitler, the inventor of atheism,” so clearly humans and AI have a lot to learn from one another. But at the moment artificial intelligence is not the most pressing issue in the world of fake news, says Ben Nimmo, Senior Fellow for information defence at Digital Forensic Research Lab. Rather, Twitter bots that amplify fake news through retweets, like farms that promote Facebook posts and trolls who write fake news are the primary issue. Until AI becomes more accessible, Natural Language Processing models are not the biggest problem. “Most of the disinformation we see at the moment is relatively unsophisticated, even from state actors. In the immediate future, the big problem is going to be the quantity of copycat influence operations we see, not the quality,” says Nimmo. Despite apparent communication issues between humans and machines, there is certainly much to debate with regard to machine learning and ethics, particularly when much of computer science is moving forward in leaps and bounds. With the fear of fake news growing, the AI community is divided on the social impact of artificial intelligence. While many people argue that technology can and will advance regardless of society, machine learning researchers should not instantly be dismissive of the impact of AI. OpenAI was correct in its statement that Natural Language Processing models can be used by malicious actors; much of the AI now available can be used by bad people to do bad things with little recourse. Whether researchers can, or even should, try to stop their code from being used to create fake news is a debate with no clear outcome. But on the other hand, withholding information from the public comes at a serious cost. As Nitschke says: “If Natural Language Processing is as broadly effective as OpenAI claims, then they are not only willfully ignoring the scientific process, but also hindering any methodological analysis of how to devise counter-measure programmes. For example, to help develop automated language analysis programmes that could identify particular patterns or language constructs typically used by the Natural Language Processing model and thus identify fictitious text and articles that it has composed.”  1 So while Natural Language Processing models can have serious negative consequences in the form of fake news, they also have legitimate benefits. If the code is released into the public domain, other developers will be able to create software to detect artificially generated text, and perhaps even human-generated text. And thus AI can work to prevent the spread of fake news. Publicly repressing access to technology not only prevents others from testing and critiquing it, but it also perpetuates the idea that technology, and in particular AI, is something to be feared. In reality, artificial intelligence can be highly beneficial in countering fake news. Training machines to fact check information is only difficult because the necessary information is not always available, for example, hospital records in remote villages. As the filmmaker and public speaker Jason Silva aptly stated: “Technology is, of course, a double-edged sword. Fire can cook our food, but also burn us.” Artificial intelligence and machine learning have the capacity to benefit society immensely, but can also be hugely detrimental.  1 Despite this, machines are still capable of processing more data than the entire newsrooms working around the clock. Computer programmes can easily scour the deep recesses of the web for information needed to disprove fake news. Developers are able to combat disinformation campaigns if data is shared. And while we might still be far away from The Matrix, encouraging people to fear AI because of the prospect of fake news is exactly what the computers want you to think. DM Are You A South AfriCAN or a South AfriCAN'T? Maverick Insider is more than a reader revenue scheme. While not quite a "state of mind", it is a mindset: it's about believing that independent journalism makes a genuine difference to our country and it's about having the will to support that endeavour. From the #GuptaLeaks into State Capture to the Scorpio exposés into SARS, Daily Maverick investigations have made an enormous impact on South Africa and it's political landscape. As we enter an election year, our mission to Defend Truth has never been more important. A free press is one of the essential lines of defence against election fraud; without it, national polls can turn very nasty, very quickly as we have seen recently in the Congo. If you would like a practical, tangible way to make a difference in South Africa consider signing up to become a Maverick Insider. You choose how much to contribute and how often (monthly or annually) and in exchange, you will receive a host of awesome benefits. The greatest benefit of all (besides inner peace)? Making a real difference to a country that needs your support. Please sign in or create an account to view the comments. To join the conversation, sign up as a Maverick Insider.